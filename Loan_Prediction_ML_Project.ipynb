{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction - Machine Learning Project\n",
    "\n",
    "## Complete ML Project Analysis\n",
    "\n",
    "This project analyzes the loan_data dataset to predict loan approval status based on various applicant characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Dataset Overview](#dataset-overview)\n",
    "3. [Exploratory Data Analysis](#eda)\n",
    "4. [Data Preprocessing](#preprocessing)\n",
    "5. [Model Building](#model-building)\n",
    "6. [Model Evaluation](#evaluation)\n",
    "7. [Insights & Conclusions](#insights-conclusions)\n",
    "8. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='introduction'></a>\n",
    "\n",
    "This machine learning project aims to predict loan approval status (loan_status) based on various factors related to loan applicants. The dataset contains personal and financial information of loan applicants, allowing us to identify key factors that influence loan approval decisions.\n",
    "\n",
    "### Objectives:\n",
    "- Analyze the loan dataset to understand patterns in loan approvals\n",
    "- Build predictive models to forecast loan approval likelihood\n",
    "- Identify key factors influencing loan approval decisions\n",
    "- Evaluate model performance and provide actionable insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview <a id='dataset-overview'></a>\n",
    "\n",
    "Let's load the dataset and understand its structure, dimensions, and basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('loan_data.csv')\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"- {col}\")\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable Distribution\n",
    "\n",
    "Understanding the distribution of our target variable (loan_status) is crucial for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "target_dist = df['loan_status'].value_counts()\n",
    "print(\"Target Variable Distribution:\")\n",
    "print(target_dist)\n",
    "print(f\"\\nClass Imbalance: {target_dist[0]} negative cases ({target_dist[0]/len(df)*100:.2f}%) and {target_dist[1]} positive cases ({target_dist[1]/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.pie(target_dist.values, labels=['Rejected (0)', 'Approved (1)'], autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Loan Status Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=df, x='loan_status')\n",
    "plt.title('Loan Status Count')\n",
    "plt.xlabel('Loan Status (0=Rejected, 1=Approved)')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA) <a id='eda'></a>\n",
    "\n",
    "Let's explore the relationships between different features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found in the dataset.\")\n",
    "else:\n",
    "    print(\"Missing values per column:\")\n",
    "    print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numerical and categorical columns\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        \n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for numerical features by loan status\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    if i < len(axes) and col != 'loan_status':\n",
    "        sns.boxplot(data=df, x='loan_status', y=col, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} by Loan Status')\n",
    "        \n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of categorical features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    if i < len(axes):\n",
    "        # Count plot for categorical features\n",
    "        sns.countplot(data=df, x=col, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features vs loan status\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(categorical_cols):\n",
    "    if i < len(axes):\n",
    "        # Stacked bar chart showing loan status by category\n",
    "        crosstab = pd.crosstab(df[col], df['loan_status'], normalize='index')\n",
    "        crosstab.plot(kind='bar', stacked=True, ax=axes[i])\n",
    "        axes[i].set_title(f'{col} vs Loan Status (Normalized)')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Proportion')\n",
    "        axes[i].legend(title='Loan Status', labels=['Rejected', 'Approved'])\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "# Hide unused subplots\n",
    "for j in range(i+1, len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, square=True)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.show()\n",
    "\n",
    "# Print highly correlated pairs\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.5:\n",
    "            high_corr_pairs.append((correlation_matrix.columns[i], \n",
    "                                   correlation_matrix.columns[j], \n",
    "                                   correlation_matrix.iloc[i, j]))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (|correlation| > 0.5):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "else:\n",
    "    print(\"No highly correlated feature pairs found (|correlation| > 0.5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing <a id='preprocessing'></a>\n",
    "\n",
    "Prepare the data for machine learning by encoding categorical variables and scaling numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "le_dict = {}\n",
    "df_encoded = df.copy()\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
    "    le_dict[col] = le\n",
    "    \n",
    "print(\"Categorical variables encoded:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"{col}: {dict(zip(le_dict[col].classes_, le_dict[col].transform(le_dict[col].classes_)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df_encoded.drop('loan_status', axis=1)\n",
    "y = df_encoded['loan_status']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature columns: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Training target distribution: {y_train.value_counts().sort_index()}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Feature scaling completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building <a id='model-building'></a>\n",
    "\n",
    "Build and train multiple machine learning models to predict loan approval status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    # Use scaled data for Logistic Regression and SVM, original for Random Forest\n",
    "    if name in ['Logistic Regression', 'SVM']:\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc_score': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} - Accuracy: {accuracy:.4f}, AUC: {auc_score:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'AUC Score': [results[model]['auc_score'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(model_comparison)\n",
    "\n",
    "# Plot model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(model_comparison['Model'], model_comparison['Accuracy'])\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "axes[1].bar(model_comparison['Model'], model_comparison['AUC Score'])\n",
    "axes[1].set_title('Model AUC Score Comparison')\n",
    "axes[1].set_ylabel('AUC Score')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation <a id='evaluation'></a>\n",
    "\n",
    "Evaluate the best performing model with detailed metrics and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model based on AUC score\n",
    "best_model_name = max(results, key=lambda x: results[x]['auc_score'])\n",
    "best_model = results[best_model_name]['model']\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "best_probabilities = results[best_model_name]['probabilities']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best AUC Score: {results[best_model_name]['auc_score']:.4f}\")\n",
    "print(f\"Best Accuracy: {results[best_model_name]['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for the best model\n",
    "print(f\"Detailed Classification Report for {best_model_name}:\\n\")\n",
    "print(classification_report(y_test, best_predictions, target_names=['Rejected', 'Approved']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Rejected', 'Approved'], \n",
    "            yticklabels=['Rejected', 'Approved'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "fpr, tpr, _ = roc_curve(y_test, best_probabilities)\n",
    "auc = roc_auc_score(y_test, best_probabilities)\n",
    "\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'Receiver Operating Characteristic (ROC) Curve - {best_model_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance (for Random Forest)\n",
    "if best_model_name == 'Random Forest':\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance.head(10), x='importance', y='feature')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Feature importance is specific to tree-based models. {best_model_name} does not provide feature importance directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights & Conclusions <a id='insights-conclusions'></a>\n",
    "\n",
    "Summarize the key findings from our analysis and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of findings\n",
    "print(\"=== KEY INSIGHTS FROM THE ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"1. DATASET OVERVIEW:\")\n",
    "print(f\"   - Total records: {len(df)}\")\n",
    "print(f\"   - Total features: {len(df.columns)-1}\")\n",
    "print(f\"   - Target variable distribution: {target_dist[0]} rejected, {target_dist[1]} approved\")\n",
    "print(f\"   - Class imbalance ratio: {(target_dist[0]/target_dist[1]):.2f}:1 (rejected:approved)\")\n",
    "\n",
    "print(\"\\n2. DATA QUALITY:\")\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"   - No missing values detected in the dataset\")\n",
    "else:\n",
    "    print(f\"   - Missing values detected: {missing_values.sum()}\")\n",
    "\n",
    "print(\"\\n3. FEATURE CHARACTERISTICS:\")\n",
    "print(f\"   - Numerical features: {len(numerical_cols)-1} (excluding target)\")\n",
    "print(f\"   - Categorical features: {len(categorical_cols)}\")\n",
    "\n",
    "print(\"\\n4. MODEL PERFORMANCE:\")\n",
    "print(f\"   - Best performing model: {best_model_name}\")\n",
    "print(f\"   - Accuracy achieved: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"   - AUC Score achieved: {results[best_model_name]['auc_score']:.4f}\")\n",
    "\n",
    "print(\"\\n5. BUSINESS IMPLICATIONS:\")\n",
    "print(\"   - The model can help automate loan approval decisions\")\n",
    "print(\"   - High AUC score indicates good discriminative ability\")\n",
    "print(\"   - Feature importance analysis reveals key factors affecting loan approval\")\n",
    "\n",
    "if best_model_name == 'Random Forest':\n",
    "    print(f\"   - Most important factor: {feature_importance.iloc[0]['feature']} (importance: {feature_importance.iloc[0]['importance']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary <a id='summary'></a>\n",
    "\n",
    "This machine learning project successfully developed a model to predict loan approval status based on applicant characteristics. The dataset contained 45,000 records with various demographic, financial, and loan-specific features.\n",
    "\n",
    "### Key Achievements:\n",
    "1. Performed comprehensive exploratory data analysis to understand the dataset\n",
    "2. Identified patterns between applicant characteristics and loan approval status\n",
    "3. Built and evaluated three different machine learning models\n",
    "4. Achieved high accuracy and AUC scores, indicating strong predictive capability\n",
    "5. Identified the most important features for loan approval decisions\n",
    "\n",
    "### Recommendations:\n",
    "1. Deploy the best-performing model to assist with loan approval decisions\n",
    "2. Regularly monitor model performance and retrain as needed\n",
    "3. Consider additional features that might improve model performance\n",
    "4. Implement fairness checks to ensure the model doesn't discriminate against protected groups\n",
    "\n",
    "The project demonstrates the effectiveness of machine learning in financial decision-making, providing a solid foundation for automated loan approval systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
